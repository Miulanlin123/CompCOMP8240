{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4961a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 (main, Oct 10 2025, 08:52:10) [GCC 13.3.0]\n",
      "JAX: 0.4.18 Devices: [CpuDevice(id=0)]\n",
      "OS: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "CWD: /home/huy/projects/vision_transformer\n"
     ]
    }
   ],
   "source": [
    "import sys, jax, numpy as np\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"JAX:\", jax.__version__, \"Devices:\", jax.devices())\n",
    "import platform, os\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e668339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/venvs/vit310/bin/python\n",
      "JAX 0.4.18\n",
      "Flax 0.7.2\n",
      "Optax 0.1.9\n",
      "NumPy 1.26.4\n",
      "SciPy 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys, jax, flax, optax, numpy as np, scipy\n",
    "print(sys.executable)                      \n",
    "print(\"JAX\", jax.__version__)              # 0.4.18\n",
    "print(\"Flax\", flax.__version__)            # 0.7.2\n",
    "print(\"Optax\", optax.__version__)          # 0.1.9\n",
    "print(\"NumPy\", np.__version__)             # 1.26.4\n",
    "print(\"SciPy\", scipy.__version__)          # 1.10.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423626d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  393M  100  393M    0     0  9747k      0  0:00:41  0:00:41 --:--:-- 10.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  400M  100  400M    0     0  10.3M      0  0:00:38  0:00:38 --:--:-- 11.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1246M  100 1246M    0     0  10.9M      0  0:01:53  0:01:53 --:--:-- 11.7M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/projects/vit_weights\n",
    "!cd ~/projects/vit_weights\n",
    "!curl -L -O https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\n",
    "!curl -L -O https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_32.npz\n",
    "!curl -L -O https://storage.googleapis.com/vit_models/imagenet21k/ViT-L_16.npz\n",
    "\n",
    "\n",
    "# fine-tuned ImageNet-1k\n",
    "# curl -L -O https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71150454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ViT-B_16 …\n",
      "Saved to: /home/huy/projects/vit_weights/ViT-B_16.npz\n",
      "OK -> /home/huy/projects/vit_weights/ViT-B_16.npz\n"
     ]
    }
   ],
   "source": [
    "# Choose a model name exactly from this list:\n",
    "# \"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\", \"Mixer-B_16\"\n",
    "model_name = \"ViT-B_16\"\n",
    "\n",
    "# Map model_name -> direct download URL (no gsutil required)\n",
    "URLS = {\n",
    "    \"ViT-B_16\": \"https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\",\n",
    "    \"ViT-B_32\": \"https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_32.npz\",\n",
    "    \"ViT-L_16\": \"https://storage.googleapis.com/vit_models/imagenet21k/ViT-L_16.npz\",\n",
    "    \"Mixer-B_16\": \"https://storage.googleapis.com/mixer_models/imagenet21k/Mixer-B_16.npz\",\n",
    "    # (Optionally: ImageNet-1k fine-tuned head)\n",
    "    # \"ViT-B_16_ft\": \"https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\",\n",
    "}\n",
    "\n",
    "import os, urllib.request, pathlib\n",
    "weights_dir = pathlib.Path.home() / \"projects\" / \"vit_weights\"\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "dst = weights_dir / f\"{model_name}.npz\"\n",
    "\n",
    "if not dst.exists():\n",
    "    print(f\"Downloading {model_name} …\")\n",
    "    urllib.request.urlretrieve(URLS[model_name], dst.as_posix())\n",
    "    print(\"Saved to:\", dst)\n",
    "else:\n",
    "    print(\"Already exists:\", dst)\n",
    "\n",
    "# Sanity check for the next cells:\n",
    "assert dst.exists(), \"Weights file was not downloaded\"\n",
    "print(\"OK ->\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468b0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 394M\n",
      "-rw-r--r-- 1 huy huy 394M Oct 14 11:36 ViT-B_16.npz\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/huy/projects/vit_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef82083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import tqdm\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Shows the number of available devices.\n",
    "# In a CPU/GPU runtime this will be a single device.\n",
    "# In a TPU runtime this will be 8 cores.\n",
    "jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9526faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_jax loaded from: /home/huy/projects/vision_transformer/vit_jax/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "repo = os.path.expanduser('~/projects/vision_transformer')  # path to the vit_jax repo\n",
    "assert os.path.isdir(repo), f\"Repo not found: {repo}\"\n",
    "if repo not in sys.path:\n",
    "    sys.path.append(repo)\n",
    "\n",
    "from vit_jax import models_vit, checkpoint\n",
    "import vit_jax, inspect\n",
    "print(\"vit_jax loaded from:\", vit_jax.__file__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73c10168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: token\n",
       "hidden_size: 768\n",
       "image_size: 224\n",
       "num_classes: 1000\n",
       "patches:\n",
       "  size: !!python/tuple\n",
       "  - 16\n",
       "  - 16\n",
       "representation_size: null\n",
       "transformer:\n",
       "  attention_dropout_rate: 0.0\n",
       "  dropout_rate: 0.0\n",
       "  mlp_dim: 3072\n",
       "  num_heads: 12\n",
       "  num_layers: 12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_collections import ConfigDict\n",
    "\n",
    "def vit_config(kind='B', patch=16, num_classes=1000, image_size=224):\n",
    "    cfg = ConfigDict()\n",
    "\n",
    "    # patches\n",
    "    cfg.patches = ConfigDict()\n",
    "    cfg.patches.size = (patch, patch)\n",
    "\n",
    "    # transformer sub-config\n",
    "    tr = ConfigDict()\n",
    "    if kind == 'B':\n",
    "        cfg.hidden_size = 768\n",
    "        tr.num_layers = 12\n",
    "        tr.mlp_dim = 3072\n",
    "        tr.num_heads = 12\n",
    "    elif kind == 'L':\n",
    "        cfg.hidden_size = 1024\n",
    "        tr.num_layers = 24\n",
    "        tr.mlp_dim = 4096\n",
    "        tr.num_heads = 16\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'B' or 'L'\")\n",
    "\n",
    "    tr.dropout_rate = 0.0\n",
    "    tr.attention_dropout_rate = 0.0\n",
    "    cfg.transformer = tr\n",
    "\n",
    "    cfg.classifier = 'token'\n",
    "    cfg.representation_size = None\n",
    "    cfg.num_classes = num_classes\n",
    "    cfg.image_size = image_size\n",
    "    return cfg\n",
    "\n",
    "CFG = vit_config('B', 16, num_classes=1000, image_size=224)\n",
    "CFG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vit_jax import checkpoint\n",
    "repo = \"/home/huy/projects/vision_transformer\"\n",
    "ckpt = os.path.join(repo, \"ViT-B_16.npz\")   # path to the downloaded checkpoint\n",
    "params = checkpoint.load(ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint classes = 21843\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Nhận diện số lớp của checkpoint =====\n",
    "import os\n",
    "from vit_jax import checkpoint\n",
    "\n",
    "repo = \"/home/huy/projects/vision_transformer\"\n",
    "ckpt = os.path.join(repo, \"ViT-B_16.npz\")   # path to the downloaded checkpoint\n",
    "\n",
    "params_raw = checkpoint.load(ckpt)\n",
    "\n",
    "# Some checkpoints have a nested 'params' key.\n",
    "params = params_raw.get('params', params_raw)\n",
    "\n",
    "def ckpt_num_classes(p):\n",
    "    # head/kernel shape: [hidden_size, num_classes]\n",
    "    try:\n",
    "        return int(p['head']['kernel'].shape[-1])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "n_cls = ckpt_num_classes(params)\n",
    "print(\"Checkpoint classes =\", n_cls)  # 21843 (ImageNet-21k) hoặc 1000 (ImageNet-1k fine-tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212fcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: token\n",
       "hidden_size: 768\n",
       "image_size: 224\n",
       "num_classes: 21843\n",
       "patches:\n",
       "  size: !!python/tuple\n",
       "  - 16\n",
       "  - 16\n",
       "representation_size: null\n",
       "transformer:\n",
       "  attention_dropout_rate: 0.0\n",
       "  dropout_rate: 0.0\n",
       "  mlp_dim: 3072\n",
       "  num_heads: 12\n",
       "  num_layers: 12"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config model correspond to checkpoint\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "def vit_config(kind='B', patch=16, num_classes=1000, image_size=224):\n",
    "    cfg = ConfigDict()\n",
    "    cfg.patches = ConfigDict(); cfg.patches.size = (patch, patch)\n",
    "    tr = ConfigDict()\n",
    "    if kind == 'B':\n",
    "        cfg.hidden_size = 768; tr.num_layers = 12; tr.mlp_dim = 3072; tr.num_heads = 12\n",
    "    elif kind == 'L':\n",
    "        cfg.hidden_size = 1024; tr.num_layers = 24; tr.mlp_dim = 4096; tr.num_heads = 16\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'B' or 'L'\")\n",
    "    tr.dropout_rate = 0.0; tr.attention_dropout_rate = 0.0\n",
    "    cfg.transformer = tr\n",
    "    cfg.classifier = 'token'\n",
    "    cfg.representation_size = None\n",
    "    cfg.num_classes = num_classes\n",
    "    cfg.image_size = 224\n",
    "    return cfg\n",
    "\n",
    "# Nếu ckpt là 21k, đặt num_classes=21843; nếu là 1k, đặt 1000\n",
    "CFG = vit_config('B', 16, num_classes=(n_cls or 1000), image_size=224)\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b07dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape = (1, 21843)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jax, jax.numpy as jnp\n",
    "from vit_jax import models_vit\n",
    "\n",
    "model = models_vit.VisionTransformer(\n",
    "    num_classes=CFG.num_classes,\n",
    "    patches=CFG.patches,\n",
    "    transformer=CFG.transformer,\n",
    "    hidden_size=CFG.hidden_size,\n",
    "    representation_size=CFG.representation_size,\n",
    "    classifier=CFG.classifier,\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy = jnp.ones([1, CFG.image_size, CFG.image_size, 3], jnp.float32)\n",
    "variables = model.init(rng, dummy, train=False)\n",
    "\n",
    "# thay params init bằng params trong checkpoint\n",
    "variables = {**variables, 'params': params}\n",
    "\n",
    "logits = model.apply(variables, dummy, train=False)\n",
    "print(\"logits shape =\", logits.shape)   # kỳ vọng: (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94b7d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  331M  100  331M    0     0  10.2M      0  0:00:32  0:00:32 --:--:-- 11.6M\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ~/projects/vision_transformer/ViT-B_16-im21k+1k.npz \\\n",
    "  https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "659ce07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "from vit_jax import models_vit, checkpoint\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "# Config ViT-B/16 @224\n",
    "def vit_config(num_classes=10, image_size=224):\n",
    "    cfg = ConfigDict(); cfg.patches = ConfigDict(); cfg.patches.size = (16, 16)\n",
    "    tr = ConfigDict()\n",
    "    cfg.hidden_size = 768; tr.num_layers = 12; tr.mlp_dim = 3072; tr.num_heads = 12\n",
    "    tr.dropout_rate = 0.0; tr.attention_dropout_rate = 0.0\n",
    "    cfg.transformer = tr; cfg.classifier='token'; cfg.representation_size=None\n",
    "    cfg.num_classes=num_classes; cfg.image_size=image_size\n",
    "    return cfg\n",
    "\n",
    "CFG = vit_config(num_classes=10, image_size=224)  # CIFAR-10 có 10 lớp\n",
    "\n",
    "model = models_vit.VisionTransformer(\n",
    "    num_classes=CFG.num_classes,\n",
    "    patches=CFG.patches,\n",
    "    transformer=CFG.transformer,\n",
    "    hidden_size=CFG.hidden_size,\n",
    "    representation_size=CFG.representation_size,\n",
    "    classifier=CFG.classifier,\n",
    ")\n",
    "\n",
    "# Init skeleton variables\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy = jnp.ones([1, 224, 224, 3], jnp.float32)\n",
    "variables = model.init(rng, dummy, train=False)\n",
    "\n",
    "# Load backbone pretrain (21k hoặc 1k đều được vì ta sẽ thay head)\n",
    "ckpt = \"/home/huy/projects/vision_transformer/ViT-B_16.npz\"  # đường dẫn ckpt bạn có (21k hay 1k)\n",
    "params = checkpoint.load(ckpt).get('params', None) or checkpoint.load(ckpt)\n",
    "\n",
    "# Thay head -> 10 lớp, giữ backbone\n",
    "params = params.copy()\n",
    "params['head'] = {\n",
    "    'kernel': jnp.zeros([CFG.hidden_size, 10], jnp.float32),\n",
    "    'bias':   jnp.zeros([10], jnp.float32),\n",
    "}\n",
    "variables = {**variables, 'params': params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54a68afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/huy/tensorflow_datasets/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/tensorflow_datasets/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[:90%], from /home/huy/tensorflow_datasets/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/tensorflow_datasets/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/tensorflow_datasets/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[90%:], from /home/huy/tensorflow_datasets/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/tensorflow_datasets/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/tensorflow_datasets/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split test, from /home/huy/tensorflow_datasets/cifar10/3.0.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, tensorflow_datasets as tfds\n",
    "tf.config.set_visible_devices([], 'GPU')  # tránh TF chiếm GPU nếu bạn dùng GPU cho JAX\n",
    "\n",
    "IM_MEAN = tf.constant([0.485, 0.456, 0.406], tf.float32)\n",
    "IM_STD  = tf.constant([0.229, 0.224, 0.225], tf.float32)\n",
    "\n",
    "def preprocess_tf(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # Resize -> 224 (CIFAR-10 là 32 nên không cần giữ tỷ lệ)\n",
    "    image = tf.image.resize(image, [224, 224], method='bicubic')\n",
    "    image = (image - IM_MEAN) / IM_STD\n",
    "    return image, tf.cast(label, tf.int32)\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "train_ds = tfds.load('cifar10', split='train[:90%]', as_supervised=True)\n",
    "val_ds   = tfds.load('cifar10', split='train[90%:]', as_supervised=True)  # dùng 10% train làm val\n",
    "test_ds  = tfds.load('cifar10', split='test', as_supervised=True)\n",
    "\n",
    "train_ds = (train_ds.shuffle(10_000).map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "val_ds   = (val_ds.map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                   .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "test_ds  = (test_ds.map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                    .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "712e2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpew0d7g7wtfds\n",
      "INFO:absl:Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\u001b[AINFO:absl:Downloading https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz into /home/huy/projects/vision_transformer/data/tfds/downloads/cs.toronto.edu_kriz_cifar-10-binaryODHPtIjLh3oLcXirEISTO7dkzyKjRCuol6lV8Wc6C7s.tar.gz.tmp.421f9a8e3e544a06b3544645692956b9...\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Extraction completed...: 100%|██████████| 8/8 [00:16<00:00,  2.12s/ file]\n",
      "Dl Size...: 100%|██████████| 162/162 [00:16<00:00,  9.54 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 16.99s/ url]\n",
      "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]INFO:absl:Done writing /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2.incompleteJD7TRB/cifar10-train.tfrecord*. Number of examples: 50000 (shards: [50000])\n",
      "Generating splits...:  50%|█████     | 1/2 [00:07<00:07,  7.69s/ splits]INFO:absl:Done writing /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2.incompleteJD7TRB/cifar10-test.tfrecord*. Number of examples: 10000 (shards: [10000])\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[:90%], from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[90%:], from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split test, from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cifar10 downloaded and prepared to /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "PROJECT_TFDS = \"/home/huy/projects/vision_transformer/data/tfds\"  # any path \n",
    "\n",
    "train_ds = tfds.load('cifar10', split='train[:90%]',  as_supervised=True,\n",
    "                     data_dir=PROJECT_TFDS, download=True)\n",
    "val_ds   = tfds.load('cifar10', split='train[90%:]',   as_supervised=True,\n",
    "                     data_dir=PROJECT_TFDS, download=True)\n",
    "test_ds  = tfds.load('cifar10', split='test',          as_supervised=True,\n",
    "                     data_dir=PROJECT_TFDS, download=True)\n",
    "    # shows how splits can be sliced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch shape: (32, 32, 3) uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 16:18:22.907502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds, jax.numpy as jnp\n",
    "\n",
    "for x, y in tfds.as_numpy(train_ds.take(1)):\n",
    "    print(\"train batch shape:\", x.shape, x.dtype)\n",
    "    #Wrong (128, 224, 224, 3) float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0da76ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IM_MEAN = tf.constant([0.485, 0.456, 0.406], tf.float32)\n",
    "IM_STD  = tf.constant([0.229, 0.224, 0.225], tf.float32)\n",
    "IMG_SIZE = CFG.image_size  # 224 (hoặc 384 nếu dùng ckpt 384)\n",
    "\n",
    "def preprocess_tf(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method='bicubic')\n",
    "\n",
    "    # đảm bảo có 3 kênh\n",
    "    if image.shape.rank == 2:\n",
    "        image = tf.expand_dims(image, -1)\n",
    "    if image.shape[-1] == 1:\n",
    "        image = tf.repeat(image, 3, axis=-1)\n",
    "\n",
    "    image = (image - IM_MEAN) / IM_STD\n",
    "    return image, tf.cast(label, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdd840e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[:90%], from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split train[90%:], from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Load dataset info from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n",
      "INFO:absl:Reusing dataset cifar10 (/home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset cifar10 for split test, from /home/huy/projects/vision_transformer/data/tfds/cifar10/3.0.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "BATCH = 32\n",
    "PROJECT_TFDS = \"/home/huy/projects/vision_transformer/data/tfds\"\n",
    "\n",
    "train_ds = (tfds.load('cifar10', split='train[:90%]', as_supervised=True, data_dir=PROJECT_TFDS)\n",
    "              .shuffle(10_000)\n",
    "              .map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .batch(BATCH)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds = (tfds.load('cifar10', split='train[90%:]', as_supervised=True, data_dir=PROJECT_TFDS)\n",
    "            .map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(BATCH)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_ds = (tfds.load('cifar10', split='test', as_supervised=True, data_dir=PROJECT_TFDS)\n",
    "             .map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "             .batch(BATCH)\n",
    "             .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2afd5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched train shape: (32, 224, 224, 3) float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 16:23:55.899510: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds, jax.numpy as jnp\n",
    "for x, y in tfds.as_numpy(train_ds.take(1)):\n",
    "    print(\"batched train shape:\", x.shape, x.dtype)  # kỳ vọng: (B, 224, 224, 3) float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fc531dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params.copy()\n",
    "params['head'] = {\n",
    "    'kernel': jnp.zeros([CFG.hidden_size, 10], jnp.float32),\n",
    "    'bias': jnp.zeros([10], jnp.float32),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab23b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: loss=0.2673  train_acc=0.9170  val_acc=0.9230\n",
      "Epoch 02: loss=0.1976  train_acc=0.9345  val_acc=0.9250\n",
      "Epoch 03: loss=0.1800  train_acc=0.9399  val_acc=0.9266\n",
      "Epoch 04: loss=0.1688  train_acc=0.9428  val_acc=0.9242\n",
      "Epoch 05: loss=0.1616  train_acc=0.9452  val_acc=0.9228\n",
      "Epoch 06: loss=0.1559  train_acc=0.9468  val_acc=0.9264\n",
      "Epoch 07: loss=0.1515  train_acc=0.9483  val_acc=0.9240\n",
      "Epoch 08: loss=0.1473  train_acc=0.9497  val_acc=0.9244\n",
      "Epoch 09: loss=0.1440  train_acc=0.9504  val_acc=0.9246\n",
      "Epoch 10: loss=0.1421  train_acc=0.9517  val_acc=0.9224\n",
      "Test top-1 accuracy: 0.9256\n"
     ]
    }
   ],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "from jax import tree_util as jtu    # <--- dùng tree_util\n",
    "import optax, tensorflow_datasets as tfds\n",
    "\n",
    "# ----- tách params -----\n",
    "full_params = params                           # params đã build + load ckpt\n",
    "backbone_params = {k: v for k, v in full_params.items() if k != 'head'}\n",
    "\n",
    "# stop gradient cho toàn bộ backbone (trên toàn pytree)\n",
    "backbone_params = jtu.tree_map(jax.lax.stop_gradient, backbone_params)\n",
    "\n",
    "head_params = full_params['head']              \n",
    "\n",
    "# optimizer chỉ cho head\n",
    "tx = optax.sgd(learning_rate=0.1, momentum=0.9, nesterov=True)\n",
    "opt_state = tx.init(head_params)\n",
    "\n",
    "# loss chỉ nhận head; backbone là hằng số (đã stop_gradient)\n",
    "def loss_fn_head(head, x, y):\n",
    "    merged = {'head': head, **backbone_params}\n",
    "    logits = model.apply({'params': merged}, x, train=True)\n",
    "    loss = optax.softmax_cross_entropy(logits, jax.nn.one_hot(y, 10)).mean()\n",
    "    acc  = (logits.argmax(-1) == y).mean()\n",
    "    return loss, acc\n",
    "\n",
    "grad_fn = jax.value_and_grad(loss_fn_head, has_aux=True)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(head, opt_state, x, y):\n",
    "    (loss, acc), grads = grad_fn(head, x, y)\n",
    "    updates, opt_state = tx.update(grads, opt_state, head)\n",
    "    head = optax.apply_updates(head, updates)\n",
    "    return head, opt_state, loss, acc\n",
    "\n",
    "def evaluate(p_head, ds):\n",
    "    merged = {'head': p_head, **backbone_params}\n",
    "    tot = cor = 0\n",
    "    for x, y in tfds.as_numpy(ds):\n",
    "        x, y = jnp.asarray(x), jnp.asarray(y)\n",
    "        logits = model.apply({'params': merged}, x, train=False)\n",
    "        cor += (logits.argmax(-1) == y).sum()\n",
    "        tot += y.shape[0]\n",
    "    return float(cor) / tot\n",
    "\n",
    "# ===== train loop (giảm batch nếu cần, ví dụ BATCH=32/16) =====\n",
    "for epoch in range(10):\n",
    "    n, loss_sum, acc_sum = 0, 0.0, 0.0\n",
    "    for x, y in tfds.as_numpy(train_ds):\n",
    "        x, y = jnp.asarray(x), jnp.asarray(y)\n",
    "        head_params, opt_state, loss, acc = train_step(head_params, opt_state, x, y)\n",
    "        loss_sum += float(loss); acc_sum += float(acc); n += 1\n",
    "    val_acc = evaluate(head_params, val_ds)\n",
    "    print(f\"Epoch {epoch+1:02d}: loss={loss_sum/n:.4f}  train_acc={acc_sum/n:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "test_acc = evaluate(head_params, test_ds)\n",
    "print(\"Test top-1 accuracy:\", test_acc)\n",
    "\n",
    "# nếu muốn lưu lại full params:\n",
    "params = {'head': head_params, **backbone_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ad29227",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'head': head_params, **backbone_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8d9b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax: 0.4.18\n",
      "orbax-checkpoint: unknown\n"
     ]
    }
   ],
   "source": [
    "import jax, flax, pkgutil\n",
    "print(\"jax:\", jax.__version__)\n",
    "import orbax\n",
    "print(\"orbax-checkpoint:\", getattr(orbax, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "220aca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to: ckpts_vit/2025-10-15_08-46-27\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "from flax import serialization\n",
    "\n",
    "ts = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "ckpt_dir = os.path.join(\"ckpts_vit\", ts)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# 1) gộp full params (khuyên dùng)\n",
    "full_params = {'head': head_params, **backbone_params}\n",
    "\n",
    "# 2) save params & opt_state (msgpack)\n",
    "with open(os.path.join(ckpt_dir, \"params.msgpack\"), \"wb\") as f:\n",
    "    f.write(serialization.to_bytes(full_params))\n",
    "\n",
    "with open(os.path.join(ckpt_dir, \"opt_state.msgpack\"), \"wb\") as f:\n",
    "    f.write(serialization.to_bytes(opt_state))\n",
    "\n",
    "# 3) meta (bạn có thể thêm label_map, num_classes, patch_size…)\n",
    "meta = {\n",
    "    \"epoch\": 10,\n",
    "    \"test_acc\": float(test_acc),\n",
    "    \"optimizer\": {\"type\": \"sgd\", \"lr\": 0.1, \"momentum\": 0.9, \"nesterov\": True},\n",
    "    \"notes\": \"ViT head-only finetune; backbone frozen\",\n",
    "}\n",
    "with open(os.path.join(ckpt_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved checkpoint to:\", ckpt_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d42c6a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/projects/vision_transformer/ckpts_vit/2025-10-15_08-46-27\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = evaluate(head_params, val_ds)   \n",
    "print(\"Val acc after load:\", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit310 (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
